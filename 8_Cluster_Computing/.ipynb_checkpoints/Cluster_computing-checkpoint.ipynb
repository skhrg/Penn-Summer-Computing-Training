{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3517870b",
   "metadata": {},
   "source": [
    "# Cluster Computing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dca6c56",
   "metadata": {},
   "source": [
    "Physics and astronomy often invove large datasets or a large number of computations or both, for which simply running code on your personal machines will not suffice. \n",
    "You could need more firepower (eg. more cores), more memory or just different processors (eg. GPUs). This is where high-performance computing (HPC) resources or computer cluster come in. You take your code, stick it on a cluster and submit jobs to be run on the cluster remotely. Here, we'll go over the basics of how to do that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d22eaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Connecting to a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f1b98e",
   "metadata": {},
   "source": [
    "In [Section 2](https://github.com/skhrg/Penn-Summer-Computing-Training/blob/main/2_ssh_and_scp/SSH_SCP_Workbook.ipynb), we learnt to securely interact with a non-local machine. To connect to most clusters, we will use SSH as described there. For example, for those of you using *devlinlab* or NERSC (depending on your specific HPC), you will do "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693c849a",
   "metadata": {},
   "source": [
    "`ssh user@devlinlab01.physics.upenn.edu`\n",
    "\n",
    "`ssh user@perlmutter-p1.nersc.gov`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a9c83",
   "metadata": {},
   "source": [
    "This will then prompt you for a password. Remember that your terminal will not indicate that you are typing while you enter your password. If you are successful, the terminal will print out something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbbefda",
   "metadata": {},
   "source": [
    "`Last login: Wed May 4 15:58:52 2022 from xx.xxx.xxx.xxx`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4871d973",
   "metadata": {},
   "source": [
    "You are now on the **head node / login node** on the cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0960477",
   "metadata": {},
   "source": [
    "### Head / Login Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116cbb53",
   "metadata": {},
   "source": [
    "#### What is a head node and why will everyone get mad at you if you abuse it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3162f7f7",
   "metadata": {},
   "source": [
    "Most clusters are set up such that you land on a *head* or *login node* when you connect to it. This is not the location where actual jobs are run, this node will not perform your computations. It's purpose is to facilitate those computations being run on a *compute node*. **Do not use the login node to run scripts.** You might wind up preventing other people from logging in by consuming resources. This node will not have enough memory to support large jobs. Etc etc. **This is how you make everyone in your research group and beyond angry with you.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f315c3ff",
   "metadata": {},
   "source": [
    "Things it is okay to use the login node for:\n",
    "- view and edit scripts\n",
    "- view output\n",
    "- perform git actions \n",
    "- submit and managae jobs\n",
    "- spy on who else is using the cluster resources\n",
    "- Managing your environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867e155d",
   "metadata": {},
   "source": [
    "Things I will yell at you for using the login node for:\n",
    "- running jobs\n",
    "- debugging scripts (there are generally nodes for this)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e67e93",
   "metadata": {},
   "source": [
    "Grey area:\n",
    "- Running literal 0 computational overhead jobs, like plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a842a4a",
   "metadata": {},
   "source": [
    "Note that for the \"grey area\" points, it's still better to perform these actions on a compute node if you can, because ultimately, those are the nodes that will be running your jobs, not your login node. For example, *marmalade* has AMD nodes as well as Intel nodes. Use the specific compute nodes that you will run your jobs on to compile your code, because there are some differences between the two types of nodes. \n",
    "\n",
    "To perform actions on a *compute node*, see running interacttive jobs below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce39b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cores vs. Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc9beb",
   "metadata": {},
   "source": [
    "**Nodes** are effectivly a self-contained CPU. This has some memory, input/output and storage. This also has processors. The processors are sometimes referred to as **cores**, but usually, each processor is made up of a ~couple cores. \n",
    "\n",
    "The cores then share everything the node has - they share memory, I/O and storage. But for parallelisable code, you can parallelise across these cores and make them simultaneously run tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e6a326",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Home "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d599169",
   "metadata": {},
   "source": [
    "The login node will take you to your home directory. This is where you can install your code, edit your `.bashrc`, direct your output. Home directories are generally backed up regularly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449b5cd6-d88d-41ba-8696-9485d2e32ecd",
   "metadata": {},
   "source": [
    "### A note on .bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b2450-076b-4401-9fc8-62aaba04a1de",
   "metadata": {},
   "source": [
    "If you use linux a lot, you will come to rely on your `.bashrc` to configure your work environment as you like it. In terms of setting up `PATH`s, aliasing commands, and other \"convenvience\" functionality, feel free to use your `.bashrc` on an HPC in the same way. However, you should check before using your `.bashrc` to load coding environments, such as loading Python. HPCs generally have several precompiled versions of Python, as well as wheels for many packages preinstalled, and it can very quickly become very confusing what packages/versions of things are loaded. Best practice is to load a specific set of modules you need and then create/load a cond or virtual environment on top of that. This has the benifit of naturally allowing you to have different environments for different projects. What you can do is use aliases to simplify this. So for example in my niagara workspace, I do not load any software by default. However, I have `alias module-gen='module load python/3.9.8 gcc openmpi fftw'`, so I can simply type `module-gen` to load the software I generally use and then `source ~/venvs/general/bin/activate` to load the virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408d45b4",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a7a2e2",
   "metadata": {},
   "source": [
    "Depending on the cluster, a separate location is preferred for runtime output called *scratch*. This has faster input/output than the *home* directory. Depending on how often you need to read/write during a job, it's better to send stuff to scratch as opposed to home. In addition to being \"faster\" this storage is also usually more robust to frequrent I/O, and so for the longevity of the cluster this is also good practice. Finally you generally get a much bigger alotement on *scratch* then on other partitions.\n",
    "\n",
    "For most clusters, scratch is **not** backed up and can be routinely purged. However data here which is `touch`-ed in any way will not be deleted. So if you are actively working (i.e. more than once per month) it will be safe. If you know you aren't going to be working on the data, move it to Data/Project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c80fc",
   "metadata": {},
   "source": [
    "### Data/Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd59bb4",
   "metadata": {},
   "source": [
    "Again, depending on the setup of the cluster, sometimes a data directory is provided per group to share and store large quantities of data. \n",
    "\n",
    "Things in data may be accesible to your whole group by default. Check the permisions and change as needed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf8fa37",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cluster set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c9af7b",
   "metadata": {},
   "source": [
    "Now that you're logged in, how do you go about setting up you code and ensuring it runs correctly? There's a few simple places to start:\n",
    "- if it's your own, personal code, you can `scp` it onto the cluster \n",
    "- `git clone` from a repository "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29fdb3f",
   "metadata": {},
   "source": [
    "### Loading available software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d7502a",
   "metadata": {},
   "source": [
    "You might also need modules to be able to run your code, for eg. an installation of C++ or fortran or python. Most clusters will have installations of these already in place, available for you to use. A good place to start to look for these is \n",
    "\n",
    "`module avail` \n",
    "\n",
    "This prints out all available pre-installed software that you can just load onto your profile on the cluster, for eg. with \n",
    "\n",
    "`module load git/xx.xx`\n",
    "\n",
    "You can then check that you indeed have access to git now with \n",
    "\n",
    "`which git`\n",
    "\n",
    "This should print some location of where the git you are using lives `...bin/git`\n",
    "\n",
    "Useful modules on niagara and NERSC include a few versions of GCC, git, anaconda, valgrind ... The cluster helpdesk is usually responsive and will install more software to add to the list of available modules if you make a good case for it (perhaps even if you make a bad one). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153518b",
   "metadata": {},
   "source": [
    "### Saving cluster set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e567e71",
   "metadata": {},
   "source": [
    "Once you know which modules you will need and have compiled your codes based on these modules, you should save this set-up. \n",
    "Check whihc modules you currently have loaded by doing \n",
    "\n",
    "`module list`\n",
    "\n",
    "As mentioned above this can be aliased in your .bashrc. Note that module load order *can* matter although generally the loader will figure out and fix any conflicts.\n",
    "\n",
    "You should also save your anaconda environment. Someone else will tell you about that. This saves the version numbers of your python packages and loads to exact right ones for which you have compiled your code and know that it works. To export the package info for the active environment, do \n",
    "\n",
    "`conda env export > environment.yml`\n",
    "\n",
    "These are important programming practices and will save you SO much time when things invariably break or you have to move your work to a different cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408ac5b",
   "metadata": {},
   "source": [
    "### Source cluster set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344cc52f",
   "metadata": {},
   "source": [
    "When you login, the cluster usually sources your .bashrc and .bash_profile. This loads all your settings, as long as you saved them here. \n",
    "\n",
    "Some clusters are set up such that you don't need to reload these settings when you move to a compute node to run a job interactively or when you submit a job. Some aren't. Generally best practice is to explicitly load the modules you want in the job script (see below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8ffebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running jobs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd15602",
   "metadata": {},
   "source": [
    "Ideally, you'd like to test your code before try to submit a job and have to wait for it to begin, compute and possibly fail because of some bug. Clusters often have debug queues dedicated to this purpose that you can submit jobs to or directly interact with to run your code. \n",
    "\n",
    "Below I'll cover how to do both **assuming the cluster uses the SLURM queuing system.** Commands for this usually start with \"s\" and you can identify them below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a09539",
   "metadata": {},
   "source": [
    "### Interactive sessions and debugjobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e495a",
   "metadata": {},
   "source": [
    "An interactive session is how you usually run code on your own machine. This is exactly the same as a home session (i.e. you have terminal access) except that you are now on a compute node and will not piss everyone off. This is a great way to debug/dev your code.\n",
    "\n",
    "You can also run interactive sessions on clusters. The exact commands sometimes differ, but the gist is \n",
    "\n",
    "`srun -N <number of nodes requested> -c <cpus per task> -n <number of tasks> -p <name of partition / queue> -t <time in minutes> --pty bash` \n",
    "\n",
    "where you're asking SLURM to run asap (you can add a delay by specifying with more arguments) on N nodes, c cpus per task and n tasks (this is an overspecified problem, N and c or N and n or n and c are enough), on the p queue for t time and to open you a bash shell. \n",
    "\n",
    "Alternatively, you can give a similar command with an executable at the end instead of requesting a bash shell. \n",
    "\n",
    "Note some clusters also have an explicit debug queue, which generally has very short wait time but also short permitted wall time, i.e. you can't ask for a job exceeding 1 hour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d0eaf",
   "metadata": {},
   "source": [
    "There's infinite more commands you can give to specify what kind of job you want to run. [Here](https://slurm.schedmd.com/srun.html) is a pretty exhaustive list of them. You can also try looking at the documentation for other clusters because as long as they too use SLURM, the commands should tranfer. \n",
    "\n",
    "Both NERSC and niagara have documentation for this, while devlinlab does not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d831a3",
   "metadata": {},
   "source": [
    "### Submitting jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2910f05",
   "metadata": {},
   "source": [
    "Another, usually better way to run things is to submit a job to the queuing system. The system then schedules your job, it will start without further intervention and per specification, email you when it ends. You can then go do other things and still delude yourself into believing you're being productive. I highly recommend it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c034f7",
   "metadata": {},
   "source": [
    "#### SLURM specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4925b",
   "metadata": {},
   "source": [
    "To do this, we write a job submission script that specifies for example, the queue to submit to, how many nodes you want, how many cores, for how long, when to email you about the job, etc. Here's an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77477e06-d4f1-4621-8e83-03e97f223bcc",
   "metadata": {},
   "source": [
    "<code>\n",
    "    #!/bin/bash\n",
    "    #\n",
    "    #SBATCH --job-name=lcdm\n",
    "    #SBATCH --output=j_lcdm_base_mcmc.txt\n",
    "    #SBATCH --ntasks-per-node=6\n",
    "    #SBATCH --cpus-per-task=4\n",
    "    #SBATCH --nodes=1\n",
    "    #SBATCH --time=1-0:00:00\n",
    "    #SBATCH --partition=highcore\n",
    "    #SBATCH --mail-type=ALL\n",
    "    #SBATCH --mail-user=karwal@sas.upenn.edu\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12bda0d",
   "metadata": {},
   "source": [
    "Here, I'm doing the following:\n",
    "- specifying that this is a bash script\n",
    "- setting job name = lcdm\n",
    "- sending output to the file j_lcdm_base_mcmc.txt in the same directory as the one from which I launch the script \n",
    "- asking for 6 tasks per node\n",
    "- and 4 cpus per task. So in total, this takes up 24 cpus. \n",
    "- for the queue I submit to, 24 cpus easily fit into one node, so I don't need more than that. This also puts all my tasks on the same node, important for my specific code. \n",
    "- asking this to run for 1 day\n",
    "- on the highcore queue\n",
    "- requesting an email about everything - this includes job beginning, finishing, timing out, or failing\n",
    "- to my email address. For marmalade, I think this has to be a Penn address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ab3a8",
   "metadata": {},
   "source": [
    "Any line that begins with `#SBATCH` is read by SLURM. All others are taken to be bash commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eff94b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Which queue is for you? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daac12f",
   "metadata": {},
   "source": [
    "Which queue you submit to is important. You might request too many resources and your job will never be scheduled, you might not have permissions to submit to certain queues, etc. all which will delay science. Or make people mad. Or both! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a2133",
   "metadata": {},
   "source": [
    "Most clusters have a general queue you can submit to, which is good if you're not sure where to go. They'll also have high wall time queues, high memory queues, etc. You should check the documentation for the cluster to find out what's available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76771b49",
   "metadata": {},
   "source": [
    "#### Job commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e88d5",
   "metadata": {},
   "source": [
    "The commands above tell SLURM *how* I want to run my job. Here's the rest of the script telling it what I actually want to run:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb4700e-28e7-4814-b57e-8c67f1dd3392",
   "metadata": {},
   "source": [
    "<code>\n",
    "\n",
    "module load python/3.9.8 gcc openmpi fftw\n",
    "source '/project/r/rbond/jorlo/virtual_envs/general/bin/activate'\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "\n",
    "mpirun --map-by ppr:4:node  python ~/dev/minkasi_jax/fitter.py ~/dev/minkasi_jax/configs/MOOJ1142/MOOJ1142_EA10.yaml\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196e365",
   "metadata": {},
   "source": [
    "Let's use knowledge you've hopefully built over this tutorial and see that here I'm doing:\n",
    "- change directory to my home directory. That's where `~` takes you\n",
    "- source my bashrc and bash_profile. This sets gets all necessary modules, sets my python environment, makes sure anything that needs to be on my PATH is added\n",
    "- then for redundancy and to debug, I check which python, gcc and mpirun the programme will call. If these are different from what I expect, one or more of my environment variables didn't set correctly\n",
    "- change directory to the one I actually want to run my script in \n",
    "- run my parallel script. Note that I'm asking mpirun to launch 6 processes here, this matches what I told SLURM - that I want 6 tasks per node and 1 node = 6x1=6 tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4641546c",
   "metadata": {},
   "source": [
    "Remember, the job output file that we specified earlier will be in whatever location you submitted your job script from, not in the directories `~` nor in `/home/karwal/some/directory/`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e332ddf6",
   "metadata": {},
   "source": [
    "So now we have our job script. Let's save this in some file `job_script.sh`.  We submit simply by \n",
    "\n",
    "`sbatch job_script.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb0716",
   "metadata": {},
   "source": [
    "That's it. The job is submitted. SLURM will take note of the resources you have requested and will allocate them when it can. Your job will begin, you'll get an email about it beginning and again when it finishes (or crashes) (or runs out of time). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b80cca7",
   "metadata": {},
   "source": [
    "#### Submitting batch jobs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e29c1a",
   "metadata": {},
   "source": [
    "Batching jobs is a useful way to quickly parallelize code without having to use MPI when certain conditions are met. A batch job simply submits the same script (or scripts) multiple times, passing it different command line elements each time. This is useful when we want to run the same process a number of times with different outputs, and are ok with saving the results to work with them later. Batching is useful when we want to do the same thing repeatedly, and each instance will take roughly (factor of a few or so) the same time. A good example might be applying the same filter to a bunch of maps. We would write one script, let's call it <code>filter.py</code>, which accepts as a commandline input the map we want to filter, does the filtering, and then saves the map somewhere. Lets say that we've created a text file, 'maps.txt', with the paths to the maps in it, something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ded5cd-b23e-4011-a163-c97a81dc8ad0",
   "metadata": {},
   "source": [
    "<code>\n",
    "    /path/to/map_1.fits\n",
    "\n",
    "        /path/to/map_2.fits\n",
    "\n",
    "        /path/to/map_3.fits\n",
    "\n",
    "              ...\n",
    "\n",
    "        /path/to/map_80.fits\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca28c5e",
   "metadata": {},
   "source": [
    "Further let's do a little trickery in filter.py. At the top, lets include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62be5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the first command line argument as the index of the map we care about\n",
    "index = int(sys.argv[1])\n",
    "\n",
    "#Read in the list of maps: this could be done a million ways, mostly likely in reality from a fits file\n",
    "maps = numpy.loadtxt('/path/to/maps.txt')\n",
    "\n",
    "#Set the current map to be the index-th map of maps\n",
    "cur_map = maps[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124658db",
   "metadata": {},
   "source": [
    "Now we will create a batch script, which runs multiple serial jobs on a single node. The script will ask for one node and all 10 cores on that node, and will then run filter.py 10 times, each on a different map. You'll notice that 10<80, so we're going to be short: we can solve this using the handy sbatch argument --array, but first let's take a look at the submission script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1cad11-5a75-42e7-8009-98f4c4a5d331",
   "metadata": {},
   "source": [
    "<code>\n",
    "    #!/bin/bash\n",
    "    # SLURM submission script for multiple serial jobs on Niagara\n",
    "    #\n",
    "    #SBATCH --nodes=1\n",
    "    #SBATCH --ntasks-per-node=10\n",
    "    #SBATCH --time=3:00:00\n",
    "    #SBATCH --job-name my_job_name\n",
    "    #SBATCH --output=/path/to/ouput.%A_%a.out\n",
    "\n",
    "        source ~/myenv/bin/activate\n",
    "\n",
    "        iter=$(($SLURM_ARRAY_TASK_ID*10))\n",
    "        #EXECUTION COMMAND; ampersand off 10 jobs and wait\n",
    "        sub_task=$(expr $iter + 0) && python3 filter.py $sub_task &\n",
    "        sub_task=$(expr $iter + 1) && python3 filter.py $sub_task &\n",
    "        sub_task=$(expr $iter + 2) && python3 filter.py $sub_task &\n",
    "        sub_task=$(expr $iter + 3) && python3 filter.py $sub_task &\n",
    "        sub_task=$(expr $iter + 4) && python3 filter.py $sub_task &\n",
    "        sub_task=$(expr $iter + 5) && python3 filter.py $sub_task &\n",
    "        sub_task=$(expr $iter + 6) && python3 filter.py $sub_task &\n",
    "        sub_task=$(expr $iter + 7) && python3 filter.py $sub_task &\n",
    "        sub_task=$(expr $iter + 8) && python3 filter.py $sub_task &\n",
    "        sub_task=$(expr $iter + 9) && python3 filter.py $sub_task &\n",
    "\n",
    "        wait\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c7508d",
   "metadata": {},
   "source": [
    "Ok, line by line let's break down what this does. --nodes=1 asks for 1 node, and --ntasks-per-node=10 asks for 10 tasks on that node. --time sets the requested run time, and --job sets the display name for the job. --output sets the output file location. Note that A will be the job ID, while a will be the task ID, set by SLURM_ARRAY_TASK_ID. source ~/myenv/bin/activate\n",
    "activates the relevant python virtual environement for running this job. To explain the iter variable, we first have to understand what SLURM_ARRAY_TASK_ID is. To submit this job, you would do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9ed8b-061c-4df1-980d-249091c45ee3",
   "metadata": {},
   "source": [
    "`sbatch --array=0-7 array_batch_script.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b9f97",
   "metadata": {},
   "source": [
    "This will submit 8 instances ([0,...7]) of array_batch_script.sh, and crucially it will pass to that script it's ID number as a variable, SLURM_ARRAY_TASK_ID. So for example the first script will have SLURM_ARRAY_TASK_ID=0, the second SLURM_ARRAY_TASK_ID=1, and so on. We then use SLURM_ARRAY_TASK_ID to specify the which maps we want to run on. So, taking SLURM_ARRAY_TASK_ID=2 as our example, iter=20. The repeated map= lines, then, actually run the script. Each line defines a new sub_task ID, which run from 20 to 29 (i.e., iter = 20 + 0, ... 20 + 9) for the 10 tasks per node we have. It then calls python on filter.py with the sub_task ID as a command line argument, which is then used in the script to specify which map we want to run on. Now we see the whole picture: the array job with SLURM_ARRAY_TASK_ID=0 will handle maps 0 through 9, the one with SLURM_ARRAY_TASK_ID=1 will handle maps 10 through 19, and so on. The wait at the end simply tells slurm to wait until all jobs are done to terminate the script. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88037a12",
   "metadata": {},
   "source": [
    "## Useful SLURM commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5621af50",
   "metadata": {},
   "source": [
    "I have hopefully driven the point home that a cluster is a community resource. If you abuse that resource or don't follow rules, people will get mad at you. \n",
    "And what do you do to make them less mad? You cancel your offending jobs! \n",
    "\n",
    "`scancel <job ID number>` will terminate that job. \n",
    "\n",
    "`sinfo` tells you about what resources are in use on the cluster. Nodes can be:\n",
    "- alloc for completely allocated \n",
    "- mix for some allocated and some free cores on the node \n",
    "- drain for a node being shut down for maintainence, but the jobs on the node will finish first \n",
    "- down for a node out of commision and\n",
    "- idle for free nodes! That await a purpose in life! And that purpose is SCIENCE! \n",
    "\n",
    "`squeue` can be used to get info on jobs currerntly running on the cluster. You can add optional arguments to this to get more detailed info, for example \n",
    "\n",
    "`squeue -u username` tells you what jobs that user currently has running. \n",
    "\n",
    "I usually define a new command in my .bash_profile as \n",
    "\n",
    "`alias sqme=\"squeue -u karwal\"` for quickly checking on my own jobs. \n",
    "\n",
    "You can similarly do \n",
    "\n",
    "`squeue -p highcore` to check on a specific partition and so on. \n",
    "\n",
    "\n",
    "Lastly, a really useful command to check when your job is scheduled to start, end and other relevant details is \n",
    "\n",
    "`scontrol show jobid ####`\n",
    "\n",
    "This fills in a few seconds after submission, once SLURM has had a chance to schedule the job. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
